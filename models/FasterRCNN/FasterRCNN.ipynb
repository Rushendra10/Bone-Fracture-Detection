{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xjjb5JUNaZd",
        "outputId": "ce3112fa-d5d4-4eb2-b8a5-c78df57b9178"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0sld7oKOZ8t"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from typing import Tuple, Any\n",
        "from functools import lru_cache\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "from torchvision.ops import box_iou\n",
        "from torchvision.ops.boxes import box_convert\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cME5mr4GOivo"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/My Drive//FracAtlas'\n",
        "\n",
        "frac_image = sorted(glob.glob(os.path.join(data_dir, 'images/Fractured/*.jpg')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wg5Na_8wnxr"
      },
      "outputs": [],
      "source": [
        "annotation_file_path = os.path.join(data_dir, 'Annotations/COCO JSON/COCO_fracture_masks_updated.json')\n",
        "output_dir = os.path.dirname(annotation_file_path)\n",
        "train_annotation_path = os.path.join(output_dir, 'COCO_fracture_masks_train.json')\n",
        "val_annotation_path = os.path.join(output_dir, 'COCO_fracture_masks_val.json')\n",
        "test_annotation_path = os.path.join(output_dir, 'COCO_fracture_masks_test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVB_6-M3SwTA",
        "outputId": "1f604eab-a178-40c1-d320-2193a487e0a1"
      },
      "outputs": [],
      "source": [
        "# Don't rerun this cell\n",
        "# Paths to fractured images and annotation file\n",
        "annotation_file_path = os.path.join(data_dir, 'Annotations/COCO JSON/COCO_fracture_masks_updated.json')\n",
        "\n",
        "# Paths to train, validation, and test CSVs\n",
        "train_csv_path = os.path.join(data_dir, 'Utilities/Fracture Split/train.csv')\n",
        "val_csv_path = os.path.join(data_dir, 'Utilities/Fracture Split/valid.csv')\n",
        "test_csv_path = os.path.join(data_dir, 'Utilities/Fracture Split/test.csv')\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Load the COCO annotation file\n",
        "with open(annotation_file_path, 'r') as f:\n",
        "    coco_annotations = json.load(f)\n",
        "\n",
        "# Get lists of image IDs for train, val, and test sets\n",
        "train_image_ids = train_df['image_id'].tolist()\n",
        "val_image_ids = val_df['image_id'].tolist()\n",
        "test_image_ids = test_df['image_id'].tolist()\n",
        "\n",
        "# Function to split annotations by image IDs\n",
        "def split_annotations(image_ids, coco_annotations):\n",
        "    new_annotations = {\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": coco_annotations[\"categories\"]\n",
        "    }\n",
        "\n",
        "    # Map image_id to new index in the split dataset\n",
        "    image_id_map = {}\n",
        "\n",
        "    # Add images\n",
        "    for img in coco_annotations[\"images\"]:\n",
        "        if img[\"file_name\"] in image_ids:\n",
        "            image_id_map[img[\"id\"]] = len(new_annotations[\"images\"]) + 1\n",
        "            img_copy = img.copy()\n",
        "            img_copy[\"id\"] = image_id_map[img[\"id\"]]\n",
        "            new_annotations[\"images\"].append(img_copy)\n",
        "\n",
        "    # Add annotations\n",
        "    for ann in coco_annotations[\"annotations\"]:\n",
        "        if ann[\"image_id\"] in image_id_map:\n",
        "            ann_copy = ann.copy()\n",
        "            ann_copy[\"image_id\"] = image_id_map[ann[\"image_id\"]]\n",
        "            new_annotations[\"annotations\"].append(ann_copy)\n",
        "\n",
        "    return new_annotations\n",
        "\n",
        "# Create split datasets\n",
        "train_annotations = split_annotations(train_image_ids, coco_annotations)\n",
        "val_annotations = split_annotations(val_image_ids, coco_annotations)\n",
        "test_annotations = split_annotations(test_image_ids, coco_annotations)\n",
        "\n",
        "# Save the split annotation files\n",
        "output_dir = os.path.dirname(annotation_file_path)\n",
        "train_annotation_path = os.path.join(output_dir, 'COCO_fracture_masks_train.json')\n",
        "val_annotation_path = os.path.join(output_dir, 'COCO_fracture_masks_val.json')\n",
        "test_annotation_path = os.path.join(output_dir, 'COCO_fracture_masks_test.json')\n",
        "\n",
        "with open(train_annotation_path, 'w') as f:\n",
        "    json.dump(train_annotations, f, indent=4)\n",
        "\n",
        "with open(val_annotation_path, 'w') as f:\n",
        "    json.dump(val_annotations, f, indent=4)\n",
        "\n",
        "with open(test_annotation_path, 'w') as f:\n",
        "    json.dump(test_annotations, f, indent=4)\n",
        "\n",
        "print(\"Annotations successfully split and saved!\")\n",
        "print(f\"Train annotations saved to: {train_annotation_path}\")\n",
        "print(f\"Validation annotations saved to: {val_annotation_path}\")\n",
        "print(f\"Test annotations saved to: {test_annotation_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teUA6TaB6lhK",
        "outputId": "d924829f-aab9-4ec8-ea12-2f462734789e"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GppVKckBblZF",
        "outputId": "6a90e865-245b-40b9-c66f-881ad7a56e40"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "# Load the annotation file\n",
        "with open(annotation_file_path, \"r\") as f:\n",
        "    coco_annotations = json.load(f)\n",
        "\n",
        "# Category information\n",
        "categories = coco_annotations['categories']  # List of category dictionaries\n",
        "category_ids = [cat['id'] for cat in categories]\n",
        "category_names = [cat['name'] for cat in categories]\n",
        "\n",
        "print(f\"Number of classes (excluding background): {len(categories)}\")\n",
        "print(\"Category IDs:\", category_ids)\n",
        "print(\"Category Names:\", category_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMsojlzv6wfX",
        "outputId": "cf757d04-4d24-4a42-d38c-70b0b2939197"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "import json\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "\n",
        "class CustomCOCODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, annotation_file, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Load COCO annotations\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "        self.images = coco_data['images']\n",
        "        self.annotations = coco_data['annotations']\n",
        "        self.image_id_to_ann = {img['id']: [] for img in self.images}\n",
        "        for ann in self.annotations:\n",
        "            self.image_id_to_ann[ann['image_id']].append(ann)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_info = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        # Load annotations\n",
        "        ann_list = self.image_id_to_ann[img_info['id']]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in ann_list:\n",
        "            # Convert from [x_min, y_min, width, height] to [x_min, y_min, x_max, y_max]\n",
        "            bbox = ann['bbox']\n",
        "            boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {'boxes': boxes, 'labels': labels}\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "# Paths\n",
        "data_dir = '/content/drive/My Drive/FracAtlas'\n",
        "fractured_image_dir = os.path.join(data_dir, 'images/Fractured')\n",
        "annotation_file_train = os.path.join(data_dir, 'Annotations/COCO JSON/COCO_fracture_masks_train.json')\n",
        "annotation_file_test = os.path.join(data_dir, 'Annotations/COCO JSON/COCO_fracture_masks_test.json')\n",
        "annotation_file_val = os.path.join(data_dir, 'Annotations/COCO JSON/COCO_fracture_masks_val.json')\n",
        "\n",
        "\n",
        "# Initialize the dataset and dataloader\n",
        "dataset_train = CustomCOCODataset(fractured_image_dir, annotation_file_train, transforms=F.to_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "dataset_val = CustomCOCODataset(fractured_image_dir, annotation_file_val, transforms=F.to_tensor)\n",
        "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "\n",
        "dataset_test = CustomCOCODataset(fractured_image_dir, annotation_file_test, transforms=F.to_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "num_classes = 2 #fracure + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "num_classes = 2\n",
        "\n",
        "# Run inference on a single batch\n",
        "threshold = 0.5  # Confidence threshold\n",
        "for imgs, targets in train_loader:\n",
        "    imgs = [img.to(device) for img in imgs]\n",
        "    outputs = model(imgs)  # Outputs will be a list of dictionaries, one per image\n",
        "\n",
        "    for i, output in enumerate(outputs):\n",
        "        print(f\"Image {i+1} predictions:\")\n",
        "        # Filter boxes based on the confidence threshold\n",
        "        boxes = output['boxes'][output['scores'] > threshold]\n",
        "        scores = output['scores'][output['scores'] > threshold]\n",
        "        labels = output['labels'][output['scores'] > threshold]\n",
        "        print(f\"Boxes: {boxes}, Scores: {scores}, Labels: {labels}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "44DBAcVB80sx",
        "outputId": "4d4613c5-9be6-43ea-e1a8-53053840b3ab"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "\n",
        "def plot_image_with_boxes(img, original_boxes, predicted_boxes, predicted_scores, labels, class_names=None):\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = F.to_pil_image(img)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Draw original bbox\n",
        "    for box in original_boxes:\n",
        "        x_min, y_min, x_max, y_max = box.tolist()\n",
        "        rect = patches.Rectangle(\n",
        "            (x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "            linewidth=2, edgecolor='blue', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 10, \"Ground Truth\", color='blue', fontsize=10)\n",
        "\n",
        "    # Draw predicted bbox\n",
        "    for box, score, label in zip(predicted_boxes, predicted_scores, labels):\n",
        "        x_min, y_min, x_max, y_max = box.tolist()\n",
        "        rect = patches.Rectangle(\n",
        "            (x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "            linewidth=2, edgecolor='red', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        label_text = f\"{class_names[label] if class_names else label} ({score:.2f})\"\n",
        "        ax.text(x_min, y_min - 10, label_text, color='red', fontsize=10)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "#Plotting a single image with its bounding boxes\n",
        "for imgs, targets in train_loader:\n",
        "    imgs = [img.to(device) for img in imgs]\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    # Get predictions\n",
        "    outputs = model(imgs)\n",
        "\n",
        "    for i, output in enumerate(outputs):\n",
        "        img = imgs[i].cpu()\n",
        "        original_boxes = targets[i]['boxes'].cpu()\n",
        "        predicted_boxes = output['boxes'].cpu()\n",
        "        predicted_scores = output['scores'].cpu()\n",
        "        predicted_labels = output['labels'].cpu()\n",
        "\n",
        "        # Display the image with bounding boxes\n",
        "        plot_image_with_boxes(\n",
        "            img,\n",
        "            original_boxes,\n",
        "            predicted_boxes,\n",
        "            predicted_scores,\n",
        "            predicted_labels,\n",
        "            class_names=None  # Replace with your class names if available\n",
        "        )\n",
        "        break  # Display only the first image for demonstration\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MqJrVFS81MI"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Load the pre-trained Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "num_classes = 2  # fractured and non-fractured\n",
        "# Replace the classifier head\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL2Jfb3X9uBn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ljG_EVI7k5x",
        "outputId": "fdbe9c3d-1bd8-481c-d984-e1d76b0bb102"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Ensure the checkpoints directory exists\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for imgs, targets in train_loader:\n",
        "        imgs = [img.to(device) for img in imgs]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(imgs, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_train_loss += losses.item()\n",
        "\n",
        "    # Average training loss for the epoch\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in val_loader:\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Switch temporarily to training mode to compute loss\n",
        "            model.train()  # Enable training mode for loss computation\n",
        "            loss_dict = model(imgs, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            model.eval()  # Switch back to evaluation mode\n",
        "\n",
        "            epoch_val_loss += losses.item()\n",
        "\n",
        "    # Average validation loss for the epoch\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save the model at the end of each epoch\n",
        "\n",
        "    checkpoint_path = f\"checkpoints/epoch_{epoch+1}.pth\"\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Model saved at: {checkpoint_path}\")\n",
        "\n",
        "# Plot the training and validation losses\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', color='b', label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, marker='o', color='r', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "B1gB07kI3NGr",
        "outputId": "4c46ad4d-fafb-47ac-cf56-204ac9874c20"
      },
      "outputs": [],
      "source": [
        "# Number of classes and device setup\n",
        "num_classes = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "model.to(device)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Compute mean IoU for a given checkpoint\n",
        "def compute_mean_iou_for_checkpoint(checkpoint_path, val_loader):\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    mean_ious = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in val_loader:  # Use the validation loader\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Forward pass to get predictions\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            for i in range(len(outputs)):  # Process each image in the batch\n",
        "                pred_boxes = outputs[i]['boxes'].cpu().numpy()  # Predicted bounding boxes\n",
        "                gt_boxes = targets[i]['boxes'].cpu().numpy()    # Ground truth bounding boxes\n",
        "\n",
        "                if len(pred_boxes) > 0 and len(gt_boxes) > 0:  # Ensure there are boxes to compare\n",
        "                    # Calculate IoU between predicted and ground truth boxes\n",
        "                    iou_matrix = box_iou(torch.tensor(pred_boxes), torch.tensor(gt_boxes))\n",
        "\n",
        "                    # Average all IoUs for this image\n",
        "                    mean_iou = iou_matrix.mean().item()\n",
        "                    mean_ious.append(mean_iou)\n",
        "\n",
        "    # Calculate the average mean IoU across all images in the test set\n",
        "    if mean_ious:\n",
        "        avg_mean_iou = np.mean(mean_ious)\n",
        "        return avg_mean_iou\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# List of 20 weight file paths\n",
        "checkpoint_files = [f\"checkpoints/epoch_{i}.pth\" for i in range(1, 21)]\n",
        "\n",
        "\n",
        "# Loop over and compute IoU for each checkpoint\n",
        "ious = []\n",
        "for checkpoint_path in checkpoint_files:\n",
        "    avg_mean_iou = compute_mean_iou_for_checkpoint(checkpoint_path, val_loader)\n",
        "    ious.append(avg_mean_iou)\n",
        "    print(f\"IoU for {checkpoint_path}: {avg_mean_iou:.4f}\")\n",
        "\n",
        "# Plot the IoU scores for all 10 checkpoints\n",
        "plt.plot(range(1, 21), ious, marker='o', linestyle='-', color='b')\n",
        "plt.title('IoU for Different Checkpoints')\n",
        "plt.xlabel('Checkpoint')\n",
        "plt.ylabel('Average Mean IoU')\n",
        "plt.xticks(range(1, 21))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE211QAa1oI9",
        "outputId": "1ac2cd94-acf2-4b7a-b692-77c36d08d902"
      },
      "outputs": [],
      "source": [
        "#Load model with best weight\n",
        "\n",
        "num_classes = 2\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Load the saved weights, skipping the box predictor\n",
        "checkpoint_path = \"checkpoints/epoch_15.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "state_dict = checkpoint\n",
        "\n",
        "# Load the modified state_dict\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "print(f\"Model weights loaded successfully from {checkpoint_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itVVhh0ltZ11"
      },
      "outputs": [],
      "source": [
        "dataset_test = CustomCOCODataset(fractured_image_dir, annotation_file_test, transforms=F.to_tensor)\n",
        "test_loader = DataLoader(dataset_test, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ62yIfxOQYA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "def visualize_boxes(img, original_boxes, predicted_boxes, predicted_scores, predicted_labels, class_names=None):\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = F.to_pil_image(img)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Draw original bounding boxes in blue\n",
        "    for box in original_boxes:\n",
        "        x_min, y_min, x_max, y_max = box.tolist()\n",
        "        rect = patches.Rectangle(\n",
        "            (x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "            linewidth=2, edgecolor='blue', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 10, \"Ground Truth\", color='blue', fontsize=10)\n",
        "\n",
        "    # Draw predicted bounding boxes in red\n",
        "    for box, score, label in zip(predicted_boxes, predicted_scores, predicted_labels):\n",
        "        x_min, y_min, x_max, y_max = box.tolist()\n",
        "        rect = patches.Rectangle(\n",
        "            (x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "            linewidth=2, edgecolor='red', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        label_text = f\"{class_names[label] if class_names else label} ({score:.2f})\"\n",
        "        ax.text(x_min, y_min - 10, label_text, color='red', fontsize=10)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "v3mtry4c2e_B",
        "outputId": "76db19be-2396-4ab3-af56-126106f99a37"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, targets in train_loader:\n",
        "        imgs = [img.to(device) for img in imgs]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Get predictions\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        for i, output in enumerate(outputs):\n",
        "            img = imgs[i].cpu()\n",
        "            original_boxes = targets[i]['boxes'].cpu()\n",
        "            predicted_boxes = output['boxes'].cpu()\n",
        "            predicted_scores = output['scores'].cpu()\n",
        "\n",
        "            # Custom model does NOT return labels yet\n",
        "            predicted_labels = torch.zeros(len(predicted_boxes), dtype=torch.int64)\n",
        "\n",
        "            plot_image_with_boxes(\n",
        "                img,\n",
        "                original_boxes,\n",
        "                predicted_boxes,\n",
        "                predicted_scores,\n",
        "                predicted_labels,\n",
        "                class_names=None\n",
        "            )\n",
        "            break\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "CkU_Ox11Kq2C",
        "outputId": "101e38b2-f618-4d57-9316-a130bc928859"
      },
      "outputs": [],
      "source": [
        "mean_ious = []\n",
        "\n",
        "# Running on the test set and calculate IoU\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, targets in test_loader:\n",
        "        imgs = [img.to(device) for img in imgs]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass to get predictions\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        for i in range(len(outputs)):\n",
        "            pred_boxes = outputs[i]['boxes'].cpu().numpy()\n",
        "            gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "\n",
        "\n",
        "            # Calculate IoU between all predicted and ground truth boxes\n",
        "            iou_matrix = box_iou(torch.tensor(pred_boxes), torch.tensor(gt_boxes))\n",
        "\n",
        "            # Include IoU of 0\n",
        "            unmatched_pred_ious = [0] * (len(pred_boxes) - iou_matrix.size(0))\n",
        "            unmatched_gt_ious = [0] * (len(gt_boxes) - iou_matrix.size(1))\n",
        "\n",
        "            all_ious = iou_matrix.flatten().tolist() + unmatched_pred_ious + unmatched_gt_ious\n",
        "            mean_iou = np.mean(all_ious)\n",
        "\n",
        "            mean_ious.append(mean_iou)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "avg_mean_iou = np.mean(mean_ious)\n",
        "print(f\"Average Mean IoU across all test images: {avg_mean_iou:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "plt.hist(mean_ious, bins=50, alpha=0.7, color='blue')\n",
        "plt.title(\"Distribution of Mean IoU\")\n",
        "plt.xlabel(\"Mean IoU\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "90JsysVH0c_2",
        "outputId": "098ae54b-fa5b-4399-8013-4326f79eb45f"
      },
      "outputs": [],
      "source": [
        "# Calculate mAP\n",
        "iou_threshold = 0.5\n",
        "num_classes = 2\n",
        "\n",
        "all_detections = {cls: {'tp': [], 'fp': [], 'scores': []} for cls in range(num_classes)}\n",
        "all_gts = {cls: [] for cls in range(num_classes)}\n",
        "\n",
        "# Run on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, targets in test_loader:\n",
        "        imgs = [img.to(device) for img in imgs]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Get predictions\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        for i in range(len(outputs)):\n",
        "            pred_boxes = outputs[i]['boxes'].cpu().numpy()  # Predicted bounding boxes\n",
        "            pred_scores = outputs[i]['scores'].cpu().numpy()  # Predicted scores\n",
        "            pred_labels = outputs[i]['labels'].cpu().numpy()  # Predicted class labels\n",
        "\n",
        "            gt_boxes = targets[i]['boxes'].cpu().numpy()    # Gt bounding boxes\n",
        "            gt_labels = targets[i]['labels'].cpu().numpy()  # Gt class labels\n",
        "\n",
        "            for label in range(num_classes):\n",
        "                pred_class_boxes = pred_boxes[pred_labels == label]\n",
        "                pred_class_scores = pred_scores[pred_labels == label]\n",
        "                gt_class_boxes = gt_boxes[gt_labels == label]\n",
        "\n",
        "                # Compute IoU for each pair of predicted and gt boxes\n",
        "                iou_matrix = box_iou(torch.tensor(pred_class_boxes), torch.tensor(gt_class_boxes))\n",
        "\n",
        "                # Check if it's a TP or FP based on IoU threshold\n",
        "                for j, pred_box in enumerate(pred_class_boxes):\n",
        "                    iou_values = iou_matrix[j]  # IoU values with all ground truth boxes for this prediction\n",
        "                    max_iou = iou_values.max().item()\n",
        "\n",
        "                    if max_iou >= iou_threshold:  # If the IoU exceeds the threshold, it's a TP. Otherwise, it's a FP\n",
        "                        all_detections[label]['tp'].append(1)\n",
        "                        all_detections[label]['fp'].append(0)\n",
        "                    else:\n",
        "                        all_detections[label]['tp'].append(0)\n",
        "                        all_detections[label]['fp'].append(1)\n",
        "\n",
        "                    all_detections[label]['scores'].append(pred_scores[j])\n",
        "\n",
        "                # Count the ground truth boxes for the current class\n",
        "                all_gts[label].extend([1] * len(gt_class_boxes))\n",
        "\n",
        "\n",
        "mean_average_precisions = []\n",
        "\n",
        "for cls in range(num_classes):\n",
        "    if len(all_gts[cls]) == 0:  # Skip classes with no ground truth instances\n",
        "        continue\n",
        "\n",
        "    # Sort detections by score\n",
        "    detections = sorted(zip(all_detections[cls]['scores'], all_detections[cls]['tp'], all_detections[cls]['fp']),\n",
        "                        key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Extract sorted values\n",
        "    sorted_scores, sorted_tp, sorted_fp = zip(*detections)\n",
        "\n",
        "    # Compute cumulative TP and FP counts\n",
        "    cum_tp = np.cumsum(sorted_tp)\n",
        "    cum_fp = np.cumsum(sorted_fp)\n",
        "\n",
        "    # Compute precision and recall\n",
        "    precision = cum_tp / (cum_tp + cum_fp + 1e-6)\n",
        "    recall = cum_tp / (len(all_gts[cls]) + 1e-6)\n",
        "\n",
        "    mean_precision = np.mean(precision)\n",
        "    mean_recall = np.mean(recall)\n",
        "\n",
        "    # Compute average precision by integrating the precision-recall curve\n",
        "    # Calculate the area under the precision-recall curve\n",
        "    ap = np.trapz(precision, recall)\n",
        "    mean_average_precisions.append(ap)\n",
        "\n",
        "# Compute mAP: mean of AP values for all classes\n",
        "mAP = np.mean(mean_average_precisions)\n",
        "print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
        "print(f\"precision: {mean_precision:.4f}\")\n",
        "print(f\"recall: {mean_recall:.4f}\")\n",
        "\n",
        "\n",
        "plt.plot(recall, precision, label=f'Class {cls}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "fbn8I182OkgO",
        "outputId": "f11cacfa-4a42-4f26-9082-cc3a7fbd1da6"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/drive/My Drive/FracAtlas/images/Fractured/IMG0002086.jpg'\n",
        "image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
        "\n",
        "# Transform the image to a tensor\n",
        "img_tensor = F.to_tensor(image).to(device).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Define hooks\n",
        "feature_maps = []\n",
        "gradients = []\n",
        "\n",
        "def forward_hook(module, input, output):\n",
        "    feature_maps.append(output)\n",
        "\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    gradients.append(grad_out[0])\n",
        "\n",
        "\n",
        "target_layer = model.backbone.body.layer4[-1]  # Last layer of ResNet50\n",
        "hook = target_layer.register_forward_hook(forward_hook)\n",
        "grad_hook = target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "\n",
        "img_list = [img_tensor.squeeze(0)]  # Remove batch dimension\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(img_list)\n",
        "\n",
        "\n",
        "scores = outputs[0]['scores']\n",
        "boxes = outputs[0]['boxes']\n",
        "target_idx = torch.argmax(scores)  # Index of highest score\n",
        "target_box = boxes[target_idx]\n",
        "\n",
        "\n",
        "# Backward pass to calculate gradients\n",
        "loss = scores[target_idx]  # Use the score of the selected box\n",
        "model.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "if gradients and feature_maps:\n",
        "    # Get the feature map and gradient\n",
        "    feature_map = feature_maps[0][0]  # First image in batch\n",
        "    gradient = gradients[0][0]\n",
        "\n",
        "    # Compute weights\n",
        "    weights = gradient.mean(dim=(1, 2), keepdim=True)  # Average over spatial dimensions\n",
        "\n",
        "    # Compute Grad-CAM\n",
        "    cam = (weights * feature_map).sum(dim=0).detach().cpu().numpy()  # Detach before converting to numpy\n",
        "\n",
        "    # Normalize CAM to [0, 1]\n",
        "    cam = np.maximum(cam, 0)  # ReLU to remove negative values\n",
        "    if cam.max() != 0:  # Avoid division by zero\n",
        "        cam = cam / cam.max()\n",
        "\n",
        "    # Resize Grad-CAM to match input image size\n",
        "    cam_resized = cv2.resize(cam, (img_tensor.shape[-1], img_tensor.shape[-2]))\n",
        "\n",
        "    # Convert original image tensor to PIL format\n",
        "    original_image = to_pil_image(img_tensor.squeeze().cpu())\n",
        "\n",
        "    # Convert CAM to heatmap\n",
        "    heatmap = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
        "\n",
        "    # Overlay heatmap on the original image\n",
        "    overlayed_image = cv2.addWeighted(np.array(original_image), 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    # Plot the images with bounding boxes\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    # Plot the original image with ground truth and predicted box\n",
        "    ax[0].imshow(original_image)\n",
        "    ax[0].set_title(\"Original Image with Boxes\")\n",
        "    ax[0].axis(\"off\")\n",
        "\n",
        "\n",
        "\n",
        "    # Add predicted box in red\n",
        "    x_min, y_min, x_max, y_max = target_box.detach().cpu().numpy()\n",
        "    rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                              linewidth=2, edgecolor='red', facecolor='none', label='Prediction')\n",
        "    ax[0].add_patch(rect)\n",
        "\n",
        "    ax[0].legend(loc=\"upper left\")\n",
        "\n",
        "    # Plot the heatmap overlay\n",
        "    ax[1].imshow(overlayed_image)\n",
        "    ax[1].set_title(\"Grad-CAM Heatmap\")\n",
        "    ax[1].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Remove hooks to avoid memory leaks\n",
        "hook.remove()\n",
        "grad_hook.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJfnj5lZdoda"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "general",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
