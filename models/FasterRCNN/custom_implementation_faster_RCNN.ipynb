{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fnn\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.ops import roi_align, nms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory containing FasterRCNN and FracAtlas folders\n",
    "ROOT = \"/Users/rushendra/Desktop/UMN/Courses/Fall 2025/CV/Project/Code/Final Project/Final Project/Code\"\n",
    "\n",
    "# Images inside FracAtlas/images/{Fractured, Not_Fractured}\n",
    "IMAGES_DIR = os.path.join(ROOT, \"FracAtlas\", \"images\")\n",
    "\n",
    "# JSON files inside FasterRCNN/COCO JSON\n",
    "JSON_DIR   = os.path.join(ROOT, \"FasterRCNN\", \"COCO JSON\")\n",
    "\n",
    "TRAIN_JSON = os.path.join(JSON_DIR, \"COCO_fracture_masks_train.json\")\n",
    "VAL_JSON   = os.path.join(JSON_DIR, \"COCO_fracture_masks_val.json\")\n",
    "TEST_JSON  = os.path.join(JSON_DIR, \"COCO_fracture_masks_test.json\")\n",
    "\n",
    "print(\"Images exist:\", os.path.isdir(IMAGES_DIR))\n",
    "print(\"Train JSON:\", os.path.exists(TRAIN_JSON))\n",
    "print(\"Val JSON:\", os.path.exists(VAL_JSON))\n",
    "print(\"Test JSON:\", os.path.exists(TEST_JSON))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2df249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCOCODataset(Dataset):\n",
    "    def __init__(self, image_root, annotation_file):\n",
    "        self.image_root = image_root\n",
    "\n",
    "        with open(annotation_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = data[\"images\"]\n",
    "        self.annotations = data[\"annotations\"]\n",
    "\n",
    "        # Map id â†’ annotations\n",
    "        self.image_to_anns = {img[\"id\"]: [] for img in self.images}\n",
    "        for ann in self.annotations:\n",
    "            self.image_to_anns[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        fname = img_info[\"file_name\"]\n",
    "\n",
    "        path1 = os.path.join(self.image_root, \"Fractured\", fname)\n",
    "        path2 = os.path.join(self.image_root, \"Not_Fractured\", fname)\n",
    "\n",
    "        if os.path.exists(path1):\n",
    "            img_path = path1\n",
    "        elif os.path.exists(path2):\n",
    "            img_path = path2\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image {fname} not found\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # NEW: resize to fixed shape\n",
    "        img = img.resize((800, 800))\n",
    "\n",
    "        # load annotation\n",
    "        anns = self.image_to_anns[img_info[\"id\"]]\n",
    "        boxes, labels = [], []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "\n",
    "            # scale bbox to resized image\n",
    "            scale_x = 800 / img_info[\"width\"]\n",
    "            scale_y = 800 / img_info[\"height\"]\n",
    "\n",
    "            x1 = x * scale_x\n",
    "            y1 = y * scale_y\n",
    "            x2 = (x + w) * scale_x\n",
    "            y2 = (y + h) * scale_y\n",
    "\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return TF.to_tensor(img), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomCOCODataset(IMAGES_DIR, TRAIN_JSON)\n",
    "val_dataset   = CustomCOCODataset(IMAGES_DIR, VAL_JSON)\n",
    "test_dataset  = CustomCOCODataset(IMAGES_DIR, TEST_JSON)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
    "                          collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False,\n",
    "                        collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False,\n",
    "                         collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cae3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # base resnet\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        resnet = resnet50(weights=weights)\n",
    "\n",
    "        # take everything up to the last conv feature map (C5)\n",
    "        self.body = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.out_channels = 2048 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return {\"0\": x}\n",
    "\n",
    "\n",
    "def build_faster_rcnn(num_classes: int, pretrained_backbone: bool = True):\n",
    "    # 1. backbone\n",
    "    backbone = ResNetBackbone(pretrained=pretrained_backbone)\n",
    "\n",
    "    # 2. anchor generator\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    # 3. ROI pooling over the single feature map 0\n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=[\"0\"],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2,\n",
    "    )\n",
    "\n",
    "    # 4. Faster R-CNN model using backbone + components\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,                 \n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921229e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 2  # 1 fracture + background\n",
    "model = build_faster_rcnn(num_classes=num_classes, pretrained_backbone=True)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = FasterRCNN(num_classes=2).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92877ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_boxes(img, original_boxes, predicted_boxes, predicted_scores):\n",
    "    img = TF.to_pil_image(img)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for box in original_boxes:\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        ax.add_patch(patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                       edgecolor='blue', fill=False, linewidth=2))\n",
    "\n",
    "    for box, score in zip(predicted_boxes, predicted_scores):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        ax.add_patch(patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                       edgecolor='red', fill=False, linewidth=2))\n",
    "        ax.text(x1, y1-5, f\"{score:.2f}\", color='red')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f808a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for imgs, targets in train_loader:\n",
    "    imgs = [img.to(device) for img in imgs]\n",
    "    outputs = model(imgs)\n",
    "\n",
    "    for i, output in enumerate(outputs):\n",
    "        img = imgs[i].cpu()\n",
    "        gt_boxes = targets[i][\"boxes\"].cpu()\n",
    "        pred_boxes = output[\"boxes\"].cpu()\n",
    "        pred_scores = output[\"scores\"].detach().cpu().squeeze()\n",
    "\n",
    "        plot_image_with_boxes(img, gt_boxes, pred_boxes, pred_scores)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234ffcb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    # Step-level tracking\n",
    "    \"step_loss\": [],\n",
    "    \"step_loss_classifier\": [],\n",
    "    \"step_loss_box_reg\": [],\n",
    "    \"step_loss_objectness\": [],\n",
    "    \"step_loss_rpn_box_reg\": [],\n",
    "\n",
    "    # Epoch-level tracking\n",
    "    \"epoch_loss\": [],\n",
    "    \"loss_classifier\": [],\n",
    "    \"loss_box_reg\": [],\n",
    "    \"loss_objectness\": [],\n",
    "    \"loss_rpn_box_reg\": [],\n",
    "\n",
    "    # Evaluation metrics\n",
    "    \"map50\": [],\n",
    "    \"map5095\": [],\n",
    "    \"mar\": [],\n",
    "    \"epoch_iou\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c834f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "LOG_STEP_FILE = \"training_steps.csv\"\n",
    "LOG_EPOCH_FILE = \"training_epoch_summary.csv\"\n",
    "HISTORY_JSON = \"training_history.json\"\n",
    "\n",
    "# Initialize CSVs only once\n",
    "if not os.path.exists(LOG_STEP_FILE):\n",
    "    with open(LOG_STEP_FILE, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"step\", \n",
    "            \"loss\", \"cls\", \"box\", \"obj\", \"rpn\"\n",
    "        ])\n",
    "\n",
    "if not os.path.exists(LOG_EPOCH_FILE):\n",
    "    with open(LOG_EPOCH_FILE, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"total_loss\", \"cls\", \"box\", \"obj\", \"rpn\",\n",
    "            \"map50\", \"map5095\", \"mar\"\n",
    "        ])\n",
    "\n",
    "def save_step_log(step, loss, cls, box, obj, rpn):\n",
    "    with open(LOG_STEP_FILE, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([step, loss, cls, box, obj, rpn])\n",
    "\n",
    "def save_epoch_log(epoch, loss, cls, box, obj, rpn, m50, m5095, mar):\n",
    "    with open(LOG_EPOCH_FILE, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch, loss, cls, box, obj, rpn, m50, m5095, mar])\n",
    "\n",
    "def save_full_history(history):\n",
    "    with open(HISTORY_JSON, \"w\") as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc21a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    recall = torch.cat([torch.tensor([0.0]), recall, torch.tensor([1.0])])\n",
    "    precision = torch.cat([torch.tensor([1.0]), precision, torch.tensor([0.0])])\n",
    "\n",
    "    # Make precision curve non-increasing\n",
    "    for i in range(len(precision)-1, 0, -1):\n",
    "        precision[i-1] = max(precision[i-1], precision[i])\n",
    "\n",
    "    idx = (recall[1:] != recall[:-1]).nonzero(as_tuple=True)[0]\n",
    "    ap = torch.sum((recall[idx+1] - recall[idx]) * precision[idx+1])\n",
    "    return float(ap)\n",
    "\n",
    "\n",
    "def evaluate_map(model, data_loader, device, iou_thresholds=None):\n",
    "    model.eval()\n",
    "\n",
    "    if iou_thresholds is None:\n",
    "        iou_thresholds = torch.linspace(0.50, 0.95, 10)\n",
    "\n",
    "    all_scores = []\n",
    "    all_matches = []\n",
    "    num_gts = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in data_loader:\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "                gt_boxes = tgt[\"boxes\"].cpu()\n",
    "                pred_boxes = out[\"boxes\"].cpu()\n",
    "                pred_scores = out[\"scores\"].cpu()\n",
    "\n",
    "                num_gts += len(gt_boxes)\n",
    "                if pred_boxes.numel() == 0:\n",
    "                    continue\n",
    "                \n",
    "                order = pred_scores.argsort(descending=True)\n",
    "                pred_boxes = pred_boxes[order]\n",
    "                pred_scores = pred_scores[order]\n",
    "\n",
    "                if len(gt_boxes) == 0:\n",
    "                    # no ground truths -> all predictions are false positives\n",
    "                    all_scores.append(pred_scores)\n",
    "                    all_matches.append(torch.zeros(len(pred_scores)))\n",
    "                    continue\n",
    "\n",
    "                ious = box_iou(pred_boxes, gt_boxes)\n",
    "                if ious.numel() > 0:\n",
    "                    max_ious = ious.max(dim=1).values  \n",
    "                else:\n",
    "                    max_ious = torch.zeros(len(pred_boxes)) \n",
    "\n",
    "                all_scores.append(pred_scores)\n",
    "                all_matches.append(max_ious)\n",
    "\n",
    "    if len(all_scores) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    all_scores = torch.cat(all_scores)\n",
    "    all_matches = torch.cat(all_matches)\n",
    "\n",
    "    order = all_scores.argsort(descending=True)\n",
    "    all_scores = all_scores[order]\n",
    "    all_matches = all_matches[order]\n",
    "\n",
    "    aps = []\n",
    "    for thr in iou_thresholds:\n",
    "        tp = (all_matches >= thr).float()\n",
    "        fp = 1 - tp\n",
    "\n",
    "        tp_cum = torch.cumsum(tp, 0)\n",
    "        fp_cum = torch.cumsum(fp, 0)\n",
    "\n",
    "        recall = tp_cum / num_gts\n",
    "        precision = tp_cum / (tp_cum + fp_cum + 1e-8)\n",
    "\n",
    "        aps.append(compute_ap(recall, precision))\n",
    "\n",
    "    map50 = aps[0]\n",
    "    map5095 = sum(aps) / len(aps)\n",
    "    mar = float(tp_cum[-1] / num_gts)\n",
    "\n",
    "    return map50, map5095, mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "checkpoint_path = \"fasterrcnn_checkpoint.pth\"\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_loss_classifier = []\n",
    "    epoch_loss_box_reg = []\n",
    "    epoch_loss_objectness = []\n",
    "    epoch_loss_rpn_box_reg = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for imgs, targets in pbar:\n",
    "        global_step += 1\n",
    "        \n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Extract floats\n",
    "        lc  = loss_dict[\"loss_classifier\"].item()\n",
    "        lbr = loss_dict[\"loss_box_reg\"].item()\n",
    "        lo  = loss_dict[\"loss_objectness\"].item()\n",
    "        lrpn = loss_dict[\"loss_rpn_box_reg\"].item()\n",
    "        loss_float = lc + lbr + lo + lrpn\n",
    "\n",
    "        # Save step logs to CSV\n",
    "        save_step_log(global_step, loss_float, lc, lbr, lo, lrpn)\n",
    "\n",
    "        # Track per-step in memory also\n",
    "        history[\"step_loss\"].append(loss_float)\n",
    "        history[\"step_loss_classifier\"].append(lc)\n",
    "        history[\"step_loss_box_reg\"].append(lbr)\n",
    "        history[\"step_loss_objectness\"].append(lo)\n",
    "        history[\"step_loss_rpn_box_reg\"].append(lrpn)\n",
    "\n",
    "        # Accumulate epoch stats\n",
    "        epoch_loss.append(loss_float)\n",
    "        epoch_loss_classifier.append(lc)\n",
    "        epoch_loss_box_reg.append(lbr)\n",
    "        epoch_loss_objectness.append(lo)\n",
    "        epoch_loss_rpn_box_reg.append(lrpn)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss_float:.4f}\",\n",
    "            \"cls\": f\"{lc:.3f}\",\n",
    "            \"box\": f\"{lbr:.3f}\",\n",
    "            \"obj\": f\"{lo:.3f}\",\n",
    "            \"rpn\": f\"{lrpn:.3f}\",\n",
    "        })\n",
    "\n",
    "    # Compute epoch averages\n",
    "    avg_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "    avg_lc   = sum(epoch_loss_classifier) / len(epoch_loss_classifier)\n",
    "    avg_lbr  = sum(epoch_loss_box_reg) / len(epoch_loss_box_reg)\n",
    "    avg_lo   = sum(epoch_loss_objectness) / len(epoch_loss_objectness)\n",
    "    avg_lrpn = sum(epoch_loss_rpn_box_reg) / len(epoch_loss_rpn_box_reg)\n",
    "\n",
    "    # Evaluate model\n",
    "    try:\n",
    "        map50, map5095, mar = evaluate_map(model, val_loader, device)\n",
    "    except Exception as e:\n",
    "        map50, map5095, mar = 0.0, 0.0, 0.0\n",
    "        \n",
    "    # map50, map5095, mar = evaluate_map(model, val_loader, device)\n",
    "\n",
    "    # Save epoch summary to CSV\n",
    "    save_epoch_log(epoch+1, avg_loss, avg_lc, avg_lbr, avg_lo, avg_lrpn, map50, map5095, mar)\n",
    "\n",
    "    # Save in-memory history\n",
    "    history[\"epoch_loss\"].append(avg_loss)\n",
    "    history[\"loss_classifier\"].append(avg_lc)\n",
    "    history[\"loss_box_reg\"].append(avg_lbr)\n",
    "    history[\"loss_objectness\"].append(avg_lo)\n",
    "    history[\"loss_rpn_box_reg\"].append(avg_lrpn)\n",
    "    history[\"map50\"].append(map50)\n",
    "    history[\"map5095\"].append(map5095)\n",
    "    history[\"mar\"].append(mar)\n",
    "\n",
    "    # Finally save JSON history every epoch\n",
    "    save_full_history(history)\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Checkpoint saved.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load trained weights\n",
    "checkpoint_path = \"fasterrcnn_checkpoint.pth\"\n",
    "model = build_faster_rcnn(num_classes=2, pretrained_backbone=False)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Checkpoint loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ce50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map50, map5095, mar = evaluate_map(model, val_loader, device)\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"mAP@50     : {map50:.4f}\")\n",
    "print(f\"mAP@50-95  : {map5095:.4f}\")\n",
    "print(f\"mAR        : {mar:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, data_loader, device, num_images=3, score_thresh=0.5):\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in data_loader:\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            for img, out, tgt in zip(imgs, outputs, targets):\n",
    "                if shown >= num_images:\n",
    "                    return\n",
    "\n",
    "                img_cpu = img.cpu()\n",
    "                gt_boxes = tgt[\"boxes\"].cpu()\n",
    "                pred_scores = out[\"scores\"].cpu()\n",
    "                pred_boxes = out[\"boxes\"].cpu()\n",
    "\n",
    "                keep = pred_scores >= score_thresh\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                pred_scores = pred_scores[keep]\n",
    "\n",
    "                plot_image_with_boxes(img_cpu, gt_boxes, pred_boxes, pred_scores)\n",
    "                shown += 1\n",
    "\n",
    "visualize_predictions(model, val_loader, device, num_images=5, score_thresh=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ed617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
